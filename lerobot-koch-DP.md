## Diffusion Policyå¤šç›®æ ‡ä»»åŠ¡æŠ“å–è®­ç»ƒ

**ä»»åŠ¡æè¿°**ï¼šæœ¬ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨å…·èº«æ™ºèƒ½ç³»ç»Ÿ(åŸºäºkochæœºå™¨äºº)å®Œæˆå¤šç›®æ ‡æŠ“å–ä»»åŠ¡ï¼Œæ¢ç´¢Diffusion Policyçš„è®­ç»ƒè¦ç‚¹ï¼Œå¢åŠ åœºæ™¯ç†è§£ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## æ•°æ®é‡‡é›†-å¤šç›®æ ‡æŠ“å–

1. æœºæ¢°è‡‚é‡‡ç”¨ACTè®­ç»ƒçš„æœºæ¢°è‡‚ï¼Œæ ‡å®šã€æµ‹è¯•ç­‰è§ACTæ¨¡å‹è®­ç»ƒï¼›

2. æ•°æ®é‡‡é›†ï¼Œå‡†å¤‡3ä¸ªç‰©å“ï¼šæ©¡çš®æ³¥çº¢è‰²å°ç¢—bowlã€è“è‰²cubeã€ç²‰è‰²cubeï¼Œå½“æ”¾ç½®ç‰©å“çš„ç›’å­è·ç¦»è¿œçš„æ—¶å€™ï¼Œéœ€è¦æ¨¡å‹èƒ½å¤Ÿç†è§£ï¼Œå¹¶å¤¹å–ç›’å­æ”¾è¿‘ç‚¹ï¼Œå†æŠ“å–ã€‚

3. æ•°æ®é‡‡é›†è¦æ±‚ï¼š

   é‡‡é›†ä¹‹å‰éœ€è¦å…ˆè°ƒèŠ‚æ‘„åƒå¤´è§’åº¦ï¼Œç¡®ä¿é¡¶éƒ¨è§†è§’å’Œä¾§è§†è§’å¯ä»¥æ¸…æ™°å…¨é¢çš„çœ‹æ¸…æ¥šä»åŠ¨æœºæ¢°è‡‚çš„å®Œæ•´åŠ¨ä½œï¼Œåœ¨æ•´ä¸ªæ“ä½œè¿‡ç¨‹ä¸­ï¼Œä¸»åŠ¨è‡‚å’Œæ“ä½œå‘˜ä¸èƒ½è¿›å…¥è§†é‡èŒƒå›´ã€‚

5. å½•åˆ¶è®­ç»ƒæ•°æ®

- æµ‹è¯•èƒ½å¦è·‘é€šé¥æ“ä½œ

  ```bash
  python lerobot/scripts/control_robot.py teleoperate --robot-path lerobot/configs/robot/koch.yaml
  ```

- é‡‡é›†æ•°æ®é›†

  - æ•°æ®æ¨é€åˆ°huggingfaceï¼Œå¿…é¡»åœ¨å½“å‰ç¯å¢ƒä¸­ç™»å½•

  ```bash
  huggingface-cli login --token hf_token--add-to-git-credential
  HF_USER=$(huggingface-cli whoami | head -n 1)
  echo $HF_USER
  ```

  - æ•°æ®é‡‡é›†

  ```bash
  python lerobot/scripts/control_robot.py record \      # è¿è¡Œcontrol_robot.pyä¸­recordå­å‘½ä»¤
      --robot-path lerobot/configs/robot/koch.yaml \
      --fps 30 \
      --root data \                                     # æœ¬åœ°æ–‡ä»¶å¤¹
      --repo-id $HF_USER/koch_grasp_multiple_objects. \ # æ–‡ä»¶å¤¹ä¸‹çš„ç›®å½•ï¼Œhugging faceä¸­ä¹Ÿæ˜¯è¿™ç§ç›®å½•
      --tags koch tutorial \
      --warmup-time-s 5 \                               # é¢„çƒ­æ—¶é—´ï¼Œä»¥é˜²å‰å‡ å¸§å›¾ç‰‡è´¨é‡ä¸å¥½
      --episode-time-s 20  \                            # ä¸€ä¸ªepisodeçš„æ—¶é—´ï¼Œsï¼Œé¥æ“åŠ¨ä½œåšå®Œçš„æ—¶é—´ï¼›
      --reset-time-s 10  \                              # æŠŠåœºæ™¯äººä¸ºå¤åŸçš„æ—¶é—´
      --num-episodes 150  \                             # é‡‡é›†çš„æ•°æ®é‡
      --push-to-hub 1 \                                 # æ˜¯å¦ä¸Šä¼ åˆ°hugging faceï¼Œ 0-å¦ï¼Œ1-æ˜¯
      --force-override 0                                # é‡‡é›†æ•°æ®æ˜¯å¦è¦†ç›–ä¹‹å‰çš„æ•°æ®ï¼›
  ```
  
- **â†ï¼ˆå·¦æ–¹å‘é”®ï¼‰**ï¼šå½“å‰é‡‡é›†å¤±è´¥ï¼Œé‡æ–°å½•åˆ¶ï¼ˆ**å›åˆ°èµ·å§‹çŠ¶æ€ï¼Œä¸ä¿å­˜**ï¼‰ï¼›**Enterï¼ˆå›è½¦é”®ï¼‰**ï¼šå½“å‰é‡‡é›†æˆåŠŸï¼Œä¿å­˜ä¸ºä¸€ä¸ª Episodeï¼›**q** æˆ– **Ctrl+C**ï¼šä¸­æ­¢å½•åˆ¶è¿›ç¨‹æˆ–é€€å‡ºé‡‡é›†è„šæœ¬

- åœ¨é‡‡é›†è¿‡ç¨‹ä¸­ï¼Œnum-episodes =150ï¼Œåˆ†æ‰¹é‡‡é›†å³å¯ï¼›

6. æ•°æ®å±•ç¤º

   ```bash
   python lerobot/scripts/visualize_dataset.py \
       --root data \
       --repo-id Lily-Huang/koch_clip_clay_bowl \
       --episode-index 0
   ```
   
   ![image-20250502202249515](assets/image-20250502202249515.png)

## DPè®­ç»ƒå’Œæ¨ç†å®Œæ•´æµç¨‹

1. DPæ¨¡å‹çš„è®­ç»ƒ

```bash
python lerobot/scripts/train.py \
dataset_repo_id=/Users/huangli/code/xbot/lerobot/data/Lily-Huang/koch_grasp_multiple_objects20250429 env=koch_real \
policy=DP_koch_real \
hydra.run.dir=outputs/train/koch_grasp_multiple_objects \
hydra.job.name=dp_koch_test 	\
device=mps \ # MacOSç³»ç»ŸèŠ¯ç‰‡
wandb.enable=true
```

2. `DP_koch_rea.yaml`é…ç½®ï¼Œç¬¬ä¸€æ¬¡è®­ç»ƒUnetéƒ¨åˆ†ä½¿ç”¨çš„Resenetï¼Œæ•ˆæœä¸å¥½ï¼Œ[ç¬¬äºŒæ¬¡è®­ç»ƒå‚è€ƒè¿™ä¸ªé“¾æ¥](https://github.com/box2ai-robotics/lerobot-joycon/tree/main)ï¼ŒUneté‡‡ç”¨transformerã€‚

- Diffusion Policyé€šå¸¸è¢«è®¤ä¸ºæ¯”ALoha-ACTçš„æ¨¡å‹æ›´èªæ˜æ›´å…·æœ‰æ³›åŒ–æ€§ï¼Œä½†æ˜¯è°ƒè¯•éš¾åº¦æ›´å¤§ï¼Œè®­ç»ƒä¸å®¹æ˜“æ”¶æ•›ï¼Œéœ€è¦æ›´å¤§çš„æ•°æ®å’Œæ›´å¤šçš„æ­¥æ•°ï¼Œæˆ‘ç›®å‰æ•°æ®200é›†ï¼Œå…¶ä¸­[lerobot/configs/policy/diffusion.yaml](https://github.com/box2ai-robotics/lerobot-joycon/blob/main/lerobot/configs/policy/diffusion.yaml)ä¸­æœ‰å‡ ä¸ªå»ºè®®ä¿®æ”¹çš„å‚æ•°ï¼š
  - `n_action_steps`ï¼š å¢å¤§ä»»åŠ¡æ¨ç†æ­¥é•¿åˆ°100å·¦å³ï¼Œå¯ä»¥æ±²å–ACTçš„éƒ¨åˆ†ä¼˜åŠ¿ï¼Œæ›´å¥½è®­ç»ƒä¸€äº›
  - [crop_shape](https://github.com/box2ai-robotics/lerobot-joycon/blob/main/lerobot/configs/policy/diffusion.yaml:79)ï¼šå¢å¤§éšæœºè£å‰ªå°ºå¯¸ï¼Œè¾“å…¥å›¾åƒå°ºå¯¸ä¸º480x640ï¼Œå»ºè®®ä¿®æ”¹ä¸º[440ï¼Œ 560]ï¼Œä¿ç•™æ›´å¤§çš„è§†é‡ï¼Œå¯ä»¥æ›´å¿«çš„æ”¶æ•›ä¸€äº›ï¼Œä½†æ³›åŒ–æ€§ä¼šç›¸å¯¹ä¸‹é™ï¼›
  - use_separate_rgb_encoder_per_camera: true  ï¼Œæˆ‘æœ‰2ä¸ªç›¸æœºé€‰æ‹©ä¸ºæ¯ä¸ªç›¸æœºä½¿ç”¨å•ç‹¬çš„RGBç¼–ç å™¨ï¼›

```yaml

seed: 100000  # éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤æ€§
dataset_repo_id: Lily-Huang/koch_grasp_multiple_objects20250429  # æ•°æ®é›†IDï¼ŒæŒ‡å®šè®­ç»ƒæ•°æ®æ¥æº

# ========== è®­ç»ƒå‚æ•°é…ç½® ==========
training:
  # --- è®­ç»ƒæ­¥æ•°å’Œæ£€æŸ¥ç‚¹ ---
  offline_steps: 300000  # ç¦»çº¿è®­ç»ƒæ€»æ­¥æ•°ï¼ŒDPè®ºæ–‡å»ºè®®200k-300kï¼Œæˆ‘è®­ç»ƒåˆ°300kæ—¶ï¼Œæ•ˆæœè¾ƒå¥½ï¼Œç†æƒ³æ—¶360k
  online_steps: 0        # åœ¨çº¿è®­ç»ƒæ­¥æ•°ï¼Œè®¾ä¸º0è¡¨ç¤ºä¸è¿›è¡Œåœ¨çº¿è®­ç»ƒ
  eval_freq: -1          # æ¯è®­ç»ƒ20000æ­¥è¯„ä¼°ä¸€æ¬¡æ¨¡å‹æ€§èƒ½ï¼Œå®˜ç½‘ä¸­pushtã€alohaéƒ½æœ‰å“åº”çš„ygmï¼Œè‡ªå·±é‡‡é›†çš„æ•°æ®æ²¡æœ‰å“åº”çš„ygmï¼Œå¡«-1ï¼Œè®­ç»ƒæ—¶å€™ä¸è¿›è¡Œè¯„ä¼°
  log_freq : 100         # è®­ç»ƒæ—¥å¿—è¾“å‡ºé¢‘ç‡
  save_freq: 20000       # æ¯è®­ç»ƒ20000æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹
  save_checkpoint: true  # æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹

  # --- æ•°æ®åŠ è½½å’Œæ‰¹å¤„ç† ---
  num_workers: 8         # æ•°æ®åŠ è½½çš„å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼Œå–å†³äºCPUæ ¸å¿ƒæ•°ï¼Œå¯¹GPUåˆ©ç”¨ç‡å½±å“è¾ƒå¤§éœ€è¦æ‰¾æ‰“åˆé€‚çš„å€¼
  batch_size: 32         # æ¯æ‰¹è®­ç»ƒçš„æ ·æœ¬æ•°ï¼Œå½±å“å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒç¨³å®šæ€§

  # --- ä¼˜åŒ–å™¨å‚æ•° ---
  grad_clip_norm: 10     # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
  lr: 1.0e-4             # å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°å¹…åº¦
  lr_scheduler: cosine   # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œä½™å¼¦é€€ç«å¯æé«˜æ”¶æ•›æ€§èƒ½
  lr_warmup_steps: 500   # é¢„çƒ­æ­¥æ•°ï¼Œåˆå§‹é˜¶æ®µç¼“æ…¢å¢åŠ å­¦ä¹ ç‡
  adam_betas: [0.95, 0.999]  # Adamä¼˜åŒ–å™¨çš„åŠ¨é‡å‚æ•°
  adam_eps: 1.0e-8           # Adamä¼˜åŒ–å™¨çš„æ•°å€¼ç¨³å®šæ€§å‚æ•°
  adam_weight_decay: 1.0e-6  # æƒé‡è¡°å‡ç³»æ•°ï¼Œæ§åˆ¶æ­£åˆ™åŒ–å¼ºåº¦
  online_steps_between_rollouts: 1  # åœ¨çº¿è®­ç»ƒä¸­æ¯æ¬¡rolloutä¹‹é—´çš„è®­ç»ƒæ­¥æ•°

  # --- æ—¶é—´åºåˆ—é‡‡æ ·é…ç½® ---
  # delta_timestampså®šä¹‰äº†ç›¸å¯¹äºå½“å‰æ—¶åˆ»çš„æ—¶é—´ç‚¹ï¼Œç”¨äºé‡‡æ ·è§‚å¯Ÿå’ŒåŠ¨ä½œæ•°æ®
  delta_timestamps:
    # å¯¹äºlaptopç›¸æœºï¼Œé‡‡æ ·ä»(1-n_obs_steps)åˆ°1çš„æ—¶é—´ç‚¹
    # ä¾‹å¦‚ï¼šn_obs_steps=2æ—¶ï¼Œrange(-1,1)=[-1,0]ï¼Œå³å½“å‰å¸§å’Œå‰ä¸€å¸§
    observation.images.laptop: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    observation.images.phone: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    observation.state: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]"
    # å¯¹äºåŠ¨ä½œï¼Œé‡‡æ ·ä»(1-n_obs_steps)åˆ°(1-n_obs_steps+horizon)çš„æ—¶é—´ç‚¹
    # ä¾‹å¦‚ï¼šn_obs_steps=2,horizon=16æ—¶ï¼Œrange(-1,15)=[-1,0,1,...,14]
    # è¿™åŒ…æ‹¬å†å²åŠ¨ä½œå’Œæœªæ¥éœ€é¢„æµ‹çš„åŠ¨ä½œ
    action: "[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1 - ${policy.n_obs_steps} + ${policy.horizon})]"

  drop_n_last_frames: 7  

eval:
  n_episodes: 50
  batch_size: 50

policy:
  name: diffusion

  # Input / output structure.
  n_obs_steps: 2         # è§‚å¯Ÿå†å²å¸§æ•°ï¼Œç”¨äºç†è§£ç¯å¢ƒåŠ¨æ€
  horizon: 128           # æ€»çš„è§„åˆ’æ­¥æ•°ï¼Œ
  n_action_steps: 100    # å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ­¥æ•°ï¼Œé€šå¸¸å°äºhorizon

  # --- è¾“å…¥è¾“å‡ºå½¢çŠ¶å®šä¹‰ ---
  input_shapes:
    # å›¾åƒè¾“å…¥å½¢çŠ¶ï¼š[é€šé“æ•°, é«˜åº¦, å®½åº¦]
    observation.images.laptop: [3, 480, 640]  # RGBå›¾åƒ
    observation.images.phone: [3, 480, 640]   # RGBå›¾åƒ
    observation.state: ["${env.state_dim}"]   # çŠ¶æ€å‘é‡ç»´åº¦ï¼Œä»ç¯å¢ƒé…ç½®ä¸­è·å–
  output_shapes:
    action: ["${env.action_dim}"]             # åŠ¨ä½œå‘é‡ç»´åº¦ï¼Œä»ç¯å¢ƒé…ç½®ä¸­è·å–

  # --- å½’ä¸€åŒ–é…ç½® ---
  input_normalization_modes:
    # mean_std: ä½¿ç”¨å‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œæ ‡å‡†åŒ–
    observation.images.laptop: mean_std
    observation.images.phone: mean_std
    observation.state: min_max
  output_normalization_modes:
    action: min_max  
  

  # ===== è§†è§‰éª¨å¹²ç½‘ç»œé…ç½® =====
  vision_backbone: resnet18          # è§†è§‰ç‰¹å¾æå–å™¨ç±»å‹
  crop_shape: [440,560]              # å›¾åƒè£å‰ªå¤§å°ï¼Œæé«˜ç‰¹å¾èšç„¦ï¼Œæˆ‘é‡‡é›†çš„å›¾ç‰‡ä¸é€‚åˆè¿‡åº¦è£å‰ª
  crop_is_random: false              # éšæœºè£å‰ªç”¨äºæ•°æ®å¢å¼ºï¼Œä¸­å¿ƒè£å‰ª
  pretrained_backbone_weights: null  # ä¸ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
  use_group_norm: True               # ä½¿ç”¨ç»„å½’ä¸€åŒ–è€Œéæ‰¹å½’ä¸€åŒ–ï¼Œå¯¹å°æ‰¹é‡æ›´ç¨³å®šï¼Œå¦‚æœä½¿ç”¨é¢„è®­ç»ƒæƒé‡åˆ™ä¸ºfalse
  spatial_softmax_num_keypoints: 32  # ç©ºé—´Softmaxæå–çš„å…³é”®ç‚¹æ•°é‡
  use_separate_rgb_encoder_per_camera: true  # æ˜¯å¦ä¸ºæ¯ä¸ªç›¸æœºä½¿ç”¨å•ç‹¬çš„RGBç¼–ç å™¨

  # ===== U-Netæ‰©æ•£æ¨¡å‹é…ç½® =====
  down_dims: [512, 1024, 2048]       # U-Netä¸‹é‡‡æ ·è·¯å¾„æ¯å±‚é€šé“æ•°
  kernel_size: 5                     # å·ç§¯æ ¸å¤§å°
  n_groups: 8                        # ç»„å½’ä¸€åŒ–çš„ç»„æ•°
  diffusion_step_embed_dim: 128      # æ‰©æ•£æ­¥éª¤åµŒå…¥ç»´åº¦
  use_film_scale_modulation: True    # ä½¿ç”¨FiLMæ¡ä»¶è°ƒåˆ¶

  # Transformer                      #å¢åŠ Unet transformerçš„éƒ¨åˆ†
  use_transformer: false
  n_layers: 4
  n_heads: 8
  n_embed: 512
  causal_attn: false

  # ===== å™ªå£°è°ƒåº¦å™¨é…ç½® =====
  noise_scheduler_type: DDPM         # æ‰©æ•£è¿‡ç¨‹ç±»å‹ï¼šDDPM(è®­ç»ƒ)æˆ–DDIM(æ¨ç†)ï¼Œæ¨ç†çš„æ—¶å€™é…ç½®æ–‡ä»¶æ”¹ä¸ºDDIM
  num_train_timesteps: 100           # è®­ç»ƒä¸­çš„æ‰©æ•£æ­¥æ•°
  beta_schedule: squaredcos_cap_v2   # Î²è°ƒåº¦ç­–ç•¥ï¼Œæ§åˆ¶å™ªå£°æ·»åŠ æ–¹å¼
  beta_start: 0.0001                 # Î²èµ·å§‹å€¼
  beta_end: 0.02                     # Î²ç»“æŸå€¼
  prediction_type: epsilon           # é¢„æµ‹æ¨¡å¼ï¼šé¢„æµ‹å™ªå£°è€Œéæ ·æœ¬
  clip_sample: True                  # æ˜¯å¦è£å‰ªç”Ÿæˆçš„æ ·æœ¬
  clip_sample_range: 1.0             # æ ·æœ¬è£å‰ªèŒƒå›´


  # Inference
  num_inference_steps: null  # if not provided, defaults to `num_train_timesteps`

  # Loss computation
  do_mask_loss_for_padding: false

```

2. è®­ç»ƒè¿‡ç¨‹çš„æŒ‡æ ‡å˜åŒ–

- è®­ç»ƒäº†280k stepsï¼Œlossæœ€ç»ˆåœ¨0.003-0.015ä¹‹é—´æ”¶æ•›

![image-20250510182927864](assets/image-20250510182927864.png)

## DPæ¨¡å‹æ¨ç†è¿‡ç¨‹

1. è¯»å–æ¨¡å‹çš„æœ€åä¸€ä¸ªcheckpointsè¿›è¡Œå®æ—¶æ¨ç†

- æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦æŠŠ`"noise_scheduler_type": "DDIM"`ï¼Œåœ¨`last/pretrained_model/config.json`å’Œ`config.yaml`éƒ½è¦æ”¹

```bash
python lerobot/scripts/control_robot_llm.py inference \
--robot-path lerobot/configs/robot/koch.yaml  \ 
--fps 30  \ 
--root data  \ 
--repo-id Lily-Huang/koch_grasp_multiple_objects20250429 \
--device mps \
-p outputs/train/act_koch_real/checkpoints/last/pretrained_model 

python lerobot/scripts/control_robot_llm.py inference --robot-path lerobot/configs/robot/koch.yaml --fps 30 --root data --repo-id Lily-Huang/koch_grasp_multiple_objects20250429 -p outputs/train/koch_grasp_multiple_objects/checkpoints/200000/pretrained_model
```

ğŸ“º **é¡¹ç›®ç»“æœæ¼”ç¤ºè§†é¢‘**

<table>
  <tr>
    <td style="text-align: center;">
      <strong>DP-multi-objects: å’Œ ACT æ¯”è¾ƒï¼Œå¯ä»¥æ•æ‰ â€œå¦‚æœéœ€è¦æŠŠç›’å­å¤¹è¿‘ä¸€äº›â€ è¿™ä¸ªåŠ¨ä½œ</strong><br>
      <img src="assets/multi_obj_DP-ezgif.com-optimize.gif" />
    </td>
  </tr>
</table>

2. å®šæ€§çš„æ€§èƒ½åˆ†æ

- Diffusion Policyè®­ç»ƒè¿‡ç¨‹ä¸­å»ºè®®DPçš„Unetéƒ¨åˆ†é‡‡ç”¨Transformerï¼Œå‚æ•°å°‘ï¼Œæ€§èƒ½è¾ƒå¥½ï¼›
  - Unet Resnetï¼š278Mï¼ˆtotal parametersï¼‰
  - Unet Transformerï¼š41M ï¼ˆtotal parametersï¼‰
- Unetéƒ¨åˆ†é‡‡ç”¨Resnetè®­ç»ƒåˆ°300k stepsï¼Œæ•ˆæœå¾ˆå·®ï¼ŒåŸºæœ¬æŠ“å–ä¸äº†ï¼Œåç»­æŸ¥çœ‹å‘ç°ï¼šactionå’Œstateè®­ç»ƒæ—¶å€™çš„å½’ä¸€åŒ–å’ŒçŠ¯è§„ä¸€åŒ–å†™é”™äº†ï¼Œå†™äº†mean_stdï¼Œæºä»£ç æ˜¯min_maxï¼Œè¿™æ˜¯æ¯”è¾ƒå¯èƒ½çš„åŸå› ã€‚
- Unet Transformerè®­ç»ƒåˆ°ä»260kè¿›è¡Œæ¨ç†ï¼Œå‘ç°300k stepsçš„æ—¶å€™ï¼Œæ•ˆæœå¼€å§‹å˜å¾—è¾ƒå¥½ï¼ŒåŸºæœ¬å‡†ç¡®ç‡80%ä»¥ä¸Šï¼Œè®­ç»ƒåˆ°360kè¾ƒç†æƒ³
- è¾ƒACTçš„è®­ç»ƒç»“æœå¯ä»¥å‘ç°ï¼ŒDPå¯ä»¥æ•æ‰**æ”¾ç½®ç‰©å“çš„ç›’å­è·ç¦»è¿œçš„æ—¶å€™ï¼Œå¤¹å–ç›’å­æ”¾è¿‘ç‚¹ï¼Œå†æŠ“å–**ã€‚

## Diffusion Policy (æ‰©æ•£æ¨¡å‹) æ•´æ•°æ®å¤„ç†æµç¨‹

1. åœ¨Diffusionå»ºæ¨¡çš„æ—¶å€™ï¼Œä¼šåœ¨æœºæ¢°è‡‚å¯æ‰§è¡ŒåŠ¨ä½œç©ºé—´å†…éšæœºåˆå§‹åŒ–å¾ˆå¤šä¸åŒçš„æ§åˆ¶å™ªå£°ï¼Œå½¢æˆä¸åŒçš„ç­–ç•¥ã€‚ç»è¿‡å¤šæ¬¡â€œå»å™ªâ€ä»¥åï¼Œç­›é€‰å‡ºæœ€ä¼˜çš„ç­–ç•¥ï¼Œè¿™å°±æ˜¯æ‰©æ•£æ§åˆ¶æ¨¡å‹çš„åŸºæœ¬åŸç†ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯æœ‰éå¸¸ä¸¥æ ¼çš„å…¬å¼è¯æ˜çš„ã€‚
2. Diffusion Policyæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†è§‰è¿åŠ¨ç­–ç•¥ï¼Œæºè‡ªè®ºæ–‡"Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"ã€‚å®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›æ¥é¢„æµ‹æœºå™¨äººçš„åŠ¨ä½œåºåˆ—ã€‚

### DPè®­ç»ƒè¿‡ç¨‹

è®­ç»ƒé˜¶æ®µæ•°æ®æµç¨‹ï¼š

```
1. DiffusionPolicy.forward(batch)
   â”œâ”€ å½’ä¸€åŒ–è¾“å…¥: batch = self.normalize_inputs(batch)
   â”œâ”€ å †å å¤šç›¸æœºå›¾åƒ: batch["observation.images"] = torch.stack([...], dim=-4)
   â”œâ”€ å½’ä¸€åŒ–ç›®æ ‡: batch = self.normalize_targets(batch)
   â””â”€ 2. DiffusionModel.compute_loss(batch)
       â”œâ”€ è¾“å…¥éªŒè¯å’Œå½¢çŠ¶æ£€æŸ¥
       â”œâ”€ 3. DiffusionModel._prepare_global_conditioning(batch)
       â”‚   â”œâ”€ 4. å½¢çŠ¶å˜æ¢: ä¸åŒç›¸æœºæ˜¯å¦å…±äº«resnet18ç‰¹å¾æå–å™¨
       â”‚   â”‚   â”œâ”€ å•ç‹¬ç¼–ç å™¨: "b s n ... -> n (b s) ..."
       â”‚   â”‚   â””â”€ å…±äº«ç¼–ç å™¨: "b s n ... -> (b s n) ..."
       â”‚   â”œâ”€ 5. DiffusionRgbEncoder.forward(å˜æ¢åçš„å›¾åƒ)
       â”‚   â”‚   â”œâ”€ å¯é€‰è£å‰ª
       â”‚   â”‚   â”œâ”€ ResNetéª¨å¹²ç½‘ç»œæå–ç‰¹å¾
       â”‚   â”‚   â”œâ”€ ç©ºé—´è½¯æœ€å¤§æ± åŒ–
       â”‚   â”‚   â””â”€ çº¿æ€§æŠ•å½±
       â”‚   â”œâ”€ é‡æ–°æ•´å½¢å›¾åƒç‰¹å¾
       â”‚   â””â”€ æ‹¼æ¥å¹¶æ‰å¹³åŒ–æ‰€æœ‰ç‰¹å¾ï¼Œ
       â”œâ”€ æ·»åŠ å™ªå£°åˆ°ç›®æ ‡åŠ¨ä½œ
       â”œâ”€ 6. UNetæ¨ç†é¢„æµ‹å™ªå£°æˆ–åŠ¨ä½œ
       â””â”€ è®¡ç®—æŸå¤±
```

1. æ ¸å¿ƒé…ç½®å‚æ•°è§£é‡Šï¼š

   ```python
   # è¾“å…¥/è¾“å‡ºç»“æ„
   n_obs_steps: int = 2                   # è§‚å¯Ÿå†å²æ­¥æ•°
   horizon: int = 16                      # é¢„æµ‹åŠ¨ä½œçš„æ€»é•¿åº¦
   n_action_steps: int = 8                # å®é™…æ‰§è¡Œçš„åŠ¨ä½œæ­¥æ•°
   
   # æ¨¡å‹æ¶æ„ç›¸å…³
   vision_backbone: str = "resnet18"      # è§†è§‰éª¨å¹²ç½‘ç»œ
   crop_shape: tuple[int, int] = (84, 84) # å›¾åƒè£å‰ªå°ºå¯¸
   spatial_softmax_num_keypoints: int = 32 # ç©ºé—´è½¯æœ€å¤§å…³é”®ç‚¹æ•°é‡
   
   # æ‰©æ•£è¿‡ç¨‹ç›¸å…³
   num_train_timesteps: int = 100         # è®­ç»ƒæ‰©æ•£æ—¶é—´æ­¥æ•°
   beta_schedule: str = "squaredcos_cap_v2" # å™ªå£°è°ƒåº¦ç­–ç•¥
   prediction_type: str = "epsilon"       # é¢„æµ‹ç±»å‹(å™ªå£°æˆ–æ ·æœ¬)
   ```

2. æ•°æ®ç»“æ„å’Œè¾“å…¥è¾“å‡ºæ ¼å¼

- åœ¨`forward/compute_loss`å‡½æ•°ä¸­ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰ï¼ŒbatchåŒ…å«äº†æ—¶é—´åºåˆ—çš„è§‚å¯Ÿå’ŒåŠ¨ä½œæ•°æ®ï¼š

  ```python
  # è®­ç»ƒé˜¶æ®µçš„batchç»“æ„
  batch = {
     "observation.images.laptop": [B, n_obs_steps, C, H, W],
     "observation.images.phone": [B, n_obs_steps, C, H, W],
     "observation.state": [B, n_obs_steps, state_dim],
     "action": [B, horizon, action_dim],            # ç›®æ ‡åŠ¨ä½œåºåˆ—
     "action_is_pad": [B, horizon]                  # å¡«å……æ ‡è¯†
  }
  ```

- åœ¨`select_action`å‡½æ•°ä¸­ï¼ˆæ¨ç†/æ‰§è¡Œé˜¶æ®µï¼‰ï¼ŒbatchåŒ…å«çš„æ˜¯å•ä¸ªæ—¶é—´æ­¥çš„è§‚å¯Ÿæ•°æ®ï¼š

  ```python
  # æ¨ç†/æ‰§è¡Œé˜¶æ®µçš„batchç»“æ„
  batch = {
     "observation.images.laptop": [B, C, H, W],     # é¡¶éƒ¨ç›¸æœºå›¾åƒ
     "observation.images.phone": [B, C, H, W],      # ä¾§è§†ç›¸æœºå›¾åƒ
     "observation.state": [B, state_dim],           # æœºå™¨äººçŠ¶æ€ï¼ˆå…³èŠ‚è§’åº¦ç­‰ï¼‰
     # å¯é€‰çš„ç¯å¢ƒçŠ¶æ€ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
     "observation.environment_state": [B, env_state_dim]
  }
  ```

3. Diffusion Policyçš„å®Œæ•´æ•°æ®å¤„ç†æµç¨‹å¦‚ä¸‹ï¼š

- 1ï¼‰è®­ç»ƒæ•°æ®ç»“æ„å’Œé¢„å¤„ç†

  ```python
  # è®­ç»ƒæ‰¹æ¬¡æ•°æ®ç»“æ„
  batch = {
      "observation.images.laptop": [B, n_obs_steps, C, H, W],  # ç¬¬ä¸€ä¸ªç›¸æœºè§†è§’
      "observation.images.phone": [B, n_obs_steps, C, H, W],   # ç¬¬äºŒä¸ªç›¸æœºè§†è§’
      "observation.state": [B, n_obs_steps, state_dim],        # æœºå™¨äººçŠ¶æ€
      "action": [B, horizon, action_dim],                      # ç›®æ ‡åŠ¨ä½œåºåˆ—
      "action_is_pad": [B, horizon]                            # åŠ¨ä½œå¡«å……æ ‡è¯†
  }
  
  # 1. DiffusionPolicy.forwardæ–¹æ³•å¤„ç†æµç¨‹ï¼Œâš ï¸æ³¨æ„ï¼šæœ‰ä¸¤ä¸ªforwardï¼Œå‚ç…§ä¸Šé¢çš„è®­ç»ƒç»“æ„å›¾
  def forward(self, batch):
      # å½’ä¸€åŒ–è¾“å…¥ï¼ˆå€¼åŸŸå˜æ¢ï¼Œç»´åº¦ä¸å˜ï¼‰
      batch = self.normalize_inputs(batch)
  
      # å †å å¤šç›¸æœºå›¾åƒ
      batch["observation.images"] = torch.stack([batch[k] for k in self.expected_image_keys], dim=-4)
      # ç»“æœ: [B, n_obs_steps, num_cameras, C, H, W]
  
      # å½’ä¸€åŒ–ç›®æ ‡åŠ¨ä½œ
      batch = self.normalize_targets(batch)
  
      # è®¡ç®—æŸå¤±
      loss = self.diffusion.compute_loss(batch)
      return {"loss": loss}
  ```

- 2ï¼‰å…¨å±€æ¡ä»¶å‡†å¤‡ä¸è§†è§‰ç‰¹å¾æå–

  ```python
  # DiffusionModel.compute_lossä¸­è°ƒç”¨_prepare_global_conditioning
  def _prepare_global_conditioning(self, batch):
      batch_size, n_obs_steps = batch["observation.state"].shape[:2]
      global_cond_feats = [batch["observation.state"]]  # [B, n_obs_steps, state_dim]
  
      # å¤„ç†å›¾åƒç‰¹å¾
      if self._use_images:
          if self.config.use_separate_rgb_encoder_per_camera:
              # é‡æ’å›¾åƒå¼ é‡ï¼Œç›¸æœºç»´åº¦æ”¾å‰é¢ï¼Œæ¯ä¸ªç›¸æœºä¸åŒçš„ç½‘ç»œ
              # [B, n_obs_steps, num_cameras, C, H, W] -> [num_cameras, (B*n_obs_steps), C, H, W]
              images_per_camera = einops.rearrange(batch["observation.images"], "b s n ... -> n (b s) ...")
  
              # æ¯ä¸ªç›¸æœºä½¿ç”¨ç‹¬ç«‹ç¼–ç å™¨
              # self.rgb_encoderå³ä¸ºï¼šDiffusionRgbEncoder.forward
              img_features_list = torch.cat([
                  encoder(images) 
                  for encoder, images in zip(self.rgb_encoder, images_per_camera)
              ])
              # ç»“æœ: [(B*n_obs_steps), (num_cameras*feature_dim)]
  
              # é‡æ–°æ•´å½¢å›æ‰¹æ¬¡å’Œæ—¶é—´ç»´åº¦
              # [(B*n_obs_steps), feature_dim] -> [B, n_obs_steps, (num_cameras*feature_dim)]
              img_features = einops.rearrange(
                  img_features_list, "(n b s) ... -> b s (n ...)", b=batch_size, s=n_obs_steps
              )
          else:
              # ä½¿ç”¨å…±äº«ç¼–ç å™¨ï¼Œåˆå¹¶æ‰¹æ¬¡ã€æ—¶é—´å’Œç›¸æœºç»´åº¦ï¼Œæ‰€æœ‰ç›¸æœºåŒä¸€ä¸ªç½‘ç»œ
              # [B, n_obs_steps, num_cameras, C, H, W] -> [(B*n_obs_steps*num_cameras), C, H, W]
              images = einops.rearrange(batch["observation.images"], "b s n ... -> (b s n) ...")
  
              # å›¾åƒç¼–ç 
              img_features = self.rgb_encoder(images)  # [(B*n_obs_steps*num_cameras), feature_dim]
  
              # é‡æ–°æ•´å½¢
              img_features = einops.rearrange(
                  img_features, "(b s n) ... -> b s (n ...)", b=batch_size, s=n_obs_steps
              )
              # ç»“æœ: [B, n_obs_steps, (num_cameras*feature_dim)]
  
          # æ·»åŠ åˆ°ç‰¹å¾åˆ—è¡¨
          global_cond_feats.append(img_features)
  
      # æ·»åŠ ç¯å¢ƒçŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
      if self._use_env_state:
          global_cond_feats.append(batch["observation.environment_state"])
  
      # æ‹¼æ¥ç‰¹å¾å¹¶å±•å¹³
      # [B, n_obs_steps, combined_feature_dim] -> [B, (n_obs_steps*combined_feature_dim)]
      # combined_feature_dim = num_cameras*feature_dim + state_dim
      return torch.cat(global_cond_feats, dim=-1).flatten(start_dim=1)
  ```

- 3ï¼‰å›¾åƒç¼–ç å™¨è¯¦è§£ï¼Œæ˜¯åœ¨ `_prepare_global_conditioning`è°ƒç”¨`self.rgb_encoder`

  ```python
  # DiffusionRgbEncoder.forwardæ–¹æ³•
  def forward(self, x):
      """
      è¾“å…¥: x: [B, C, H, W] å›¾åƒå¼ é‡
      è¾“å‡º: [B, feature_dim] ç‰¹å¾å‘é‡
      """
      # å¯é€‰çš„è£å‰ªé¢„å¤„ç†
      if self.do_crop:
          if self.training:
              x = self.maybe_random_crop(x)  # è®­ç»ƒæ—¶éšæœºè£å‰ª
          else:
              x = self.center_crop(x)        # æ¨ç†æ—¶ä¸­å¿ƒè£å‰ª
  
      # ResNetæå–ç‰¹å¾
      backbone_features = self.backbone(x)  # [B, 512, h', w']
  
      # ç©ºé—´è½¯æœ€å¤§æ± åŒ–æå–å…³é”®ç‚¹
      # è¾“å…¥: [B, 512, h', w']
      # è¾“å‡º: [B, num_keypoints, 2]
      pooled_features = self.pool(backbone_features)
  
      # å±•å¹³å¹¶æŠ•å½±
      x = torch.flatten(pooled_features, start_dim=1)  # [B, num_keypoints*2]
      x = self.relu(self.out(x))  # [B, feature_dim]
  
      return x
  ```

- 4ï¼‰æ‰©æ•£æ¨¡å‹æŸå¤±è®¡ç®—ï¼Œåœ¨ `DiffusionPolicy.forward`è°ƒç”¨çš„losså‡½æ•°

  ```python
  def compute_loss(self, batch):
      """è®¡ç®—æ‰©æ•£æ¨¡å‹è®­ç»ƒæŸå¤±"""
      # è¾“å…¥æ•°æ®éªŒè¯
      assert set(batch).issuperset({"observation.state", "action", "action_is_pad"})
      n_obs_steps = batch["observation.state"].shape[1]
      horizon = batch["action"].shape[1]
      assert horizon == self.config.horizon
      assert n_obs_steps == self.config.n_obs_steps
  
      # å‡†å¤‡å…¨å±€æ¡ä»¶ï¼Œç»è¿‡å›¾åƒç‰¹å¾æå–åï¼Œä¸”å’Œstateç»“åˆçš„ç‰¹å¾
      global_cond = self._prepare_global_conditioning(batch)  # [B, global_cond_dim]
  
      # è·å–ç›®æ ‡åŠ¨ä½œè½¨è¿¹
      trajectory = batch["action"]  # [B, horizon, action_dim]
  
      # éšæœºé‡‡æ ·å™ªå£°
      eps = torch.randn(trajectory.shape, device=trajectory.device)
  
      # éšæœºé‡‡æ ·å™ªå£°æ—¶é—´æ­¥
      # ç†è®ºä¸Šæ˜¯ä¸€æ­¥æ­¥åŠ å™ªå£°ï¼Œä½†å®é™…è®­ç»ƒæ—¶ï¼Œç›´æ¥æ ¹æ®éšæœºé‡‡æ ·çš„æ—¶é—´æ­¥ tï¼Œä¸€æ¬¡æ€§åŠ å¯¹åº”tæ—¶åˆ»çš„æ€»å™ªå£°é‡ã€‚
      # å™ªå£°å¼ºåº¦éšç€ t å¢å¤§è€Œå¢å¤§ã€‚
      timesteps = torch.randint(
          low=0,
          high=self.noise_scheduler.config.num_train_timesteps,
          size=(trajectory.shape[0],),
          device=trajectory.device,
      ).long()
  
      # å‰å‘æ‰©æ•£ï¼šå‘å¹²å‡€è½¨è¿¹æ·»åŠ å™ªå£°ï¼Œè¿™ä¸ªå°±å¯¹åº”DDPMè®ºæ–‡ä¸­å…¬å¼ï¼šx_t = âˆš(Î±â‚œ) xâ‚€ + âˆš(1-Î±â‚œ) Îµ
      noisy_trajectory = self.noise_scheduler.add_noise(trajectory, eps, timesteps)
  
      # UNeté¢„æµ‹ï¼ˆå™ªå£°æˆ–åŸå§‹è½¨è¿¹ï¼‰
      pred = self.unet(noisy_trajectory, timesteps, global_cond=global_cond)
  
      # æ ¹æ®é¢„æµ‹ç±»å‹é€‰æ‹©ç›®æ ‡
      if self.config.prediction_type == "epsilon": # æ­¤æ—¶UNetè¾“å‡ºä¸ºå™ªå£°é¢„æµ‹
          target = eps  # é¢„æµ‹å™ªå£°
      elif self.config.prediction_type == "sample": # æ­¤æ—¶UNetè¾“å‡ºä¸ºè½¨è¿¹é¢„æµ‹
          target = batch["action"]  # é¢„æµ‹åŸå§‹è½¨è¿¹ï¼ŒLerobot
  
      # è®¡ç®—æŸå¤±
      loss = F.mse_loss(pred, target, reduction="none")
  
      # å¯é€‰ï¼šæ©ç å¡«å……éƒ¨åˆ†çš„æŸå¤±
      if self.config.do_mask_loss_for_padding:
          in_episode_bound = ~batch["action_is_pad"]
          loss = loss * in_episode_bound.unsqueeze(-1)
  
      return loss.mean()
  ```

- 5ï¼‰æ¡ä»¶UNetç»“æ„å’Œå‰å‘ä¼ æ’­ï¼Œ`compute_loss`ä¸­çš„`self.unet`

  ```python
  class DiffusionConditionalUnet1d(nn.Module):
      def forward(self, x, timestep, global_cond=None):
          """
          è¾“å…¥:
            x: [B, horizon, action_dim] å¸¦å™ªå£°çš„åŠ¨ä½œåºåˆ—
            timestep: [B] å½“å‰æ‰©æ•£æ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ ·æœ¬å¯ä»¥éšæœºå¯¹åº”è‡ªå·±ç‹¬ç«‹çš„æ—¶é—´æ­¥xtï¼Œæ¯ä¸ªbatchä¸­çš„æ ·æœ¬æœ‰è‡ªå·±çš„t
            global_cond: [B, global_cond_dim] å…¨å±€æ¡ä»¶å‘é‡
          è¾“å‡º:
            [B, horizon, action_dim] é¢„æµ‹ç»“æœ
          """
          # è°ƒæ•´å½¢çŠ¶é€‚åº”1Då·ç§¯
          x = einops.rearrange(x, "b t d -> b d t")  # [B, action_dim, horizon]
  
          # æ—¶é—´æ­¥ç¼–ç 
          timesteps_embed = self.diffusion_step_encoder(timestep)  # [B, embed_dim]
  
          # æ‹¼æ¥æ—¶é—´æ­¥ç¼–ç å’Œå…¨å±€æ¡ä»¶
          if global_cond is not None:
              # global_feature.shap = [B, (n_cameras * feature_dim + state_dim + embed_dim)]
              global_feature = torch.cat([timesteps_embed, global_cond], axis=-1)
          else:
              global_feature = timesteps_embed
  
          # UNetç¼–ç å™¨éƒ¨åˆ†3ä¸ªencoderï¼Œç½‘ç»œè¾“å…¥æ˜¯å™ªå£°çš„åŠ¨ä½œåºåˆ—
          # global_featureæ˜¯ FiLM ä½œç”¨åœ¨ ResNet block çš„ç‰¹å¾è¾“å‡ºä¸Šï¼Œåšçš„æ˜¯é€šé“æ–¹å‘çš„çº¿æ€§ç¼©æ”¾å’Œå¹³ç§»ï¼ˆé€é€šé“è°ƒåˆ¶ï¼‰ã€‚
          # global_featureé€šè¿‡ä¸€ä¸ªMLPä¸ºæ¯ä¸ªé€šé“ç”Ÿæˆä¸€å †ç³»æ•°ï¼Œç„¶ååŠ è½½resnetçš„è¾“å‡ºç»“æœä¸Š
          #   cond_embed = self.cond_encoder(cond).unsqueeze(-1)  # condæ˜¯global_feature
          #   if self.use_film_scale_modulation:
                # å°†åµŒå…¥åˆ†ä¸ºç¼©æ”¾å’Œåç½®å‚æ•°
          #      scale = cond_embed[:, : self.out_channels]
          #      bias = cond_embed[:, self.out_channels :]
          #      out = scale * out + bias  # åŒæ—¶åº”ç”¨ç¼©æ”¾å’Œåç½®
          #   else:
          # åªåº”ç”¨åç½®
          #      out = out + cond_embed
          encoder_skip_features = []
          for resnet, resnet2, downsample in self.down_modules:
              x = resnet(x, global_feature)
              x = resnet2(x, global_feature)
              encoder_skip_features.append(x)
              x = downsample(x)  # ä¸‹é‡‡æ ·
  
          # UNetä¸­é—´éƒ¨åˆ†
          # UNetçš„ä¸­é—´éƒ¨åˆ†ï¼ˆself.mid_modulesï¼‰ç¡®å®æ˜¯ä½¿ç”¨FiLMæ¡ä»¶è°ƒåˆ¶çš„DiffusionConditionalResidualBlock1dæ¨¡å—
          # UNetçš„ä¸­é—´éƒ¨åˆ†æ˜¯ç”±ä¸¤ä¸ªå¸¦æœ‰FiLMæ¡ä»¶è°ƒåˆ¶çš„æ®‹å·®å—ç»„æˆï¼Œè¿™äº›æ®‹å·®å—å…è®¸å…¨å±€æ¡ä»¶ï¼ˆå›¾åƒç‰¹å¾ã€çŠ¶æ€å’Œæ—¶é—´æ­¥ä¿¡æ¯ï¼‰å½±å“ç‰¹å¾æ˜ å°„å¤„ç†ã€‚è¿™æ˜¯ä½¿æ¡ä»¶æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®è¾“å…¥è§‚å¯Ÿç”Ÿæˆç›¸åº”åŠ¨ä½œåºåˆ—çš„å…³é”®æœºåˆ¶ä¹‹ä¸€ã€‚
          for mid_module in self.mid_modules:
              x = mid_module(x, global_feature)
  
          # UNetè§£ç å™¨éƒ¨åˆ†ï¼ˆå¸¦è·³è·ƒè¿æ¥ï¼‰2ä¸ªdecoder
          for resnet, resnet2, upsample in self.up_modules:
              x = torch.cat((x, encoder_skip_features.pop()), dim=1)  # æ·»åŠ è·³è·ƒè¿æ¥
              x = resnet(x, global_feature)
              x = resnet2(x, global_feature)
              x = upsample(x)  # ä¸Šé‡‡æ ·
  
          x = self.final_conv(x)  # [B, action_dim, horizon]
  
          # æ¢å¤åŸå§‹å½¢çŠ¶
          x = einops.rearrange(x, "b d t -> b t d")  # [B, horizon, action_dim]
  
          return x
  ```

### DPæ¨ç†è¿‡ç¨‹

1. æ¨ç†è¿‡ç¨‹çš„æ•´ä½“æµç¨‹

   ```text
   1. DiffusionPolicy.select_action(batch)
      â”œâ”€ å½’ä¸€åŒ–è¾“å…¥: batch = self.normalize_inputs(batch)
      â”œâ”€ å †å å¤šç›¸æœºå›¾åƒ: batch["observation.images"] = torch.stack([...], dim=-4)
      â”œâ”€ æ›´æ–°è§‚å¯Ÿé˜Ÿåˆ—: self._queues = populate_queues(self._queues, batch)
      â””â”€ å¦‚æœåŠ¨ä½œé˜Ÿåˆ—ä¸ºç©º:
          â”œâ”€ ä»é˜Ÿåˆ—å †å è§‚å¯Ÿ: batch = {k: torch.stack(list(self._queues[k]), dim=1) for k...}
          â”œâ”€ 2. DiffusionModel.generate_actions(batch)
          â”‚   â”œâ”€ 3. DiffusionModel._prepare_global_conditioning(batch)
          â”‚   â”‚   â”œâ”€ 4. å½¢çŠ¶å˜æ¢:
          â”‚   â”‚   â”‚   â”œâ”€ å•ç‹¬ç¼–ç å™¨: "b s n ... -> n (b s) ..."
          â”‚   â”‚   â”‚   â””â”€ å…±äº«ç¼–ç å™¨: "b s n ... -> (b s n) ..."
          â”‚   â”‚   â”œâ”€ 5. DiffusionRgbEncoder.forward(å˜æ¢åçš„å›¾åƒ)
          â”‚   â”‚   â”‚   â”œâ”€ å¯é€‰è£å‰ª(è¯„ä¼°æ—¶ä½¿ç”¨ä¸­å¿ƒè£å‰ª)
          â”‚   â”‚   â”‚   â”œâ”€ ResNetéª¨å¹²ç½‘ç»œæå–ç‰¹å¾
          â”‚   â”‚   â”‚   â”œâ”€ ç©ºé—´è½¯æœ€å¤§æ± åŒ–
          â”‚   â”‚   â”‚   â””â”€ çº¿æ€§æŠ•å½±
          â”‚   â”‚   â”œâ”€ é‡æ–°æ•´å½¢å›¾åƒç‰¹å¾
          â”‚   â”‚   â””â”€ æ‹¼æ¥å¹¶æ‰å¹³åŒ–æ‰€æœ‰ç‰¹å¾ 
          â”‚   â”œâ”€ 6. DiffusionModel.conditional_sample(global_cond)
          â”‚   â”‚   â”œâ”€ ç”Ÿæˆåˆå§‹å™ªå£°
          â”‚   â”‚   â”œâ”€ è®¾ç½®æ‰©æ•£æ—¶é—´æ­¥
          â”‚   â”‚   â”œâ”€ å¾ªç¯å»å™ªè¿‡ç¨‹:
          â”‚   â”‚   â”‚   â”œâ”€ UNeté¢„æµ‹
          â”‚   â”‚   â”‚   â””â”€ å™ªå£°è°ƒåº¦å™¨æ­¥è¿›
          â”‚   â”‚   â””â”€ è¿”å›å»å™ªåçš„åŠ¨ä½œåºåˆ—
          â”‚   â””â”€ æå–æ‰€éœ€éƒ¨åˆ†çš„åŠ¨ä½œ
          â”œâ”€ åå½’ä¸€åŒ–åŠ¨ä½œ
          â””â”€ å°†åŠ¨ä½œæ·»åŠ åˆ°é˜Ÿåˆ—
      â””â”€ ä»é˜Ÿåˆ—è¿”å›ä¸‹ä¸€ä¸ªåŠ¨ä½œ
   ```

1. å•æ­¥è§‚å¯Ÿè¾“å…¥å¤„ç†

   ```python
   # æ¨ç†é˜¶æ®µçš„è¾“å…¥
   batch = {
     "observation.images.laptop": [B, C, H, W],  # ç¬”è®°æœ¬ç›¸æœºå›¾åƒï¼ˆå•æ—¶é—´æ­¥ï¼‰
     "observation.images.phone": [B, C, H, W],   # æ‰‹æœºç›¸æœºå›¾åƒï¼ˆå•æ—¶é—´æ­¥ï¼‰
     "observation.state": [B, state_dim],        # æœºå™¨äººçŠ¶æ€ï¼ˆå•æ—¶é—´æ­¥ï¼‰
   }
   ```

2. è§‚å¯Ÿé˜Ÿåˆ—ç®¡ç†

   ```python
   @torch.no_grad
   def select_action(self, batch):
       # å½’ä¸€åŒ–è¾“å…¥
       batch = self.normalize_inputs(batch)
   
       # å †å å¤šç›¸æœºå›¾åƒ
       batch["observation.images"] = torch.stack([batch[k] for k in self.expected_image_keys], dim=-4)
       # ç»“æœ: [B, num_cameras, C, H, W]
   
       # æ›´æ–°è§‚å¯Ÿé˜Ÿåˆ—
       self._queues = populate_queues(self._queues, batch)
       # é˜Ÿåˆ—ç»“æ„ï¼š
       # self._queues = {
       #     "observation.state": deque(maxlen=n_obs_steps),       # æœ€è¿‘n_obs_stepsæ­¥çš„çŠ¶æ€
       #     "observation.images": deque(maxlen=n_obs_steps),      # æœ€è¿‘n_obs_stepsæ­¥çš„å›¾åƒ
       #     "action": deque(maxlen=n_action_steps),               # é¢„æµ‹çš„åŠ¨ä½œåºåˆ—
       # }
   
       # å¦‚æœåŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºï¼Œç”Ÿæˆæ–°çš„åŠ¨ä½œåºåˆ—
       if len(self._queues["action"]) == 0:
           # ä»é˜Ÿåˆ—æ„å»ºåºåˆ—è¾“å…¥ï¼Œå°†é˜Ÿåˆ—ä¸­å­˜å‚¨çš„å†å²è§‚å¯Ÿæ•°æ®ï¼ˆçŠ¶æ€å’Œå›¾åƒï¼‰è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
           batch = {k: torch.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
           # ç»“æœ: 
           # "observation.state": [B, n_obs_steps, state_dim]
           # "observation.images": [B, n_obs_steps, num_cameras, C, H, W]
   
           # ç”ŸæˆåŠ¨ä½œåºåˆ—
           actions = self.diffusion.generate_actions(batch)  # [B, n_action_steps, action_dim]
   
           # åå½’ä¸€åŒ–
           actions = self.unnormalize_outputs({"action": actions})["action"]
   
           # å°†åŠ¨ä½œæ·»åŠ åˆ°é˜Ÿåˆ—
           self._queues["action"].extend(actions.transpose(0, 1))
           # è½¬ç½®å: n_action_stepsä¸ª[B, action_dim]
   
       # è¿”å›ä¸‹ä¸€ä¸ªåŠ¨ä½œ
       action = self._queues["action"].popleft()  # [B, action_dim]
       return action
   ```

3. åŠ¨ä½œç”Ÿæˆè¯¦è§£ï¼Œ`select_action`ä¸­çš„`self.diffusion.generate_actions`

   ```python
   def generate_actions(self, batch):
       batch_size, n_obs_steps = batch["observation.state"].shape[:2]
       assert n_obs_steps == self.config.n_obs_steps
   
       # å‡†å¤‡å…¨å±€æ¡ä»¶ï¼Œæ­¤æ—¶æ¨ç†çš„è¾“å…¥æ•°æ®å·²ç»ç»è¿‡reset18è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶ä¸”å’Œstateè¿›è¡Œconcat
       global_cond = self._prepare_global_conditioning(batch)  # [B, global_cond_dim]
   
       # æ¡ä»¶æ‰©æ•£é‡‡æ ·
       actions = self.conditional_sample(batch_size, global_cond=global_cond)
       # ç»“æœ: [B, horizon, action_dim]
   
       # æå–éœ€è¦çš„åŠ¨ä½œéƒ¨åˆ†
       start = n_obs_steps - 1  # ä»å½“å‰è§‚å¯Ÿå¯¹åº”çš„ä½ç½®å¼€å§‹
       end = start + self.config.n_action_steps
       actions = actions[:, start:end]  # [B, n_action_steps, action_dim]
   
       return actions
   ```

4. æ¡ä»¶æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œ`generate_actions`ä¸­çš„`self.conditional_sample`

   ```python
   # æ¨ç†çš„è¿‡ç¨‹ä¸­æ˜¯åˆå§‹åŒ–ä¸€ä¸ªå™ªéŸ³åŠ¨ä½œï¼Œå’Œè¾“å…¥çš„å›¾åƒå’Œstateæ•°æ®å½¢æˆçš„global_condç„¶åè¾“å…¥åˆ°Unetç½‘ç»œ
   # è¾“å‡ºçš„model_outputæ˜¯é€šè¿‡è®­ç»ƒå¥½çš„unetè¾“å‡ºçš„å™ªå£°
   def conditional_sample(self, batch_size, global_cond=None):
       device = get_device_from_parameters(self)
       dtype = get_dtype_from_parameters(self)
   
       # é‡‡æ ·åˆå§‹å™ªå£°ï¼Œéšæœºå™ªéŸ³
       sample = torch.randn(
           size=(batch_size, self.config.horizon, self.config.output_shapes["action"][0]),
           dtype=dtype, device=device
       )  # [B, horizon, action_dim]
   
       # è®¾ç½®æ‰©æ•£æ—¶é—´æ­¥
       self.noise_scheduler.set_timesteps(self.num_inference_steps)
   
       # é€æ­¥å»å™ª
       for t in self.noise_scheduler.timesteps:
           # é¢„æµ‹æ¨¡å‹è¾“å‡ºï¼Œself.config.prediction_type == "epsilon"ï¼Œè¾“å‡ºçš„æ˜¯å™ªéŸ³ï¼Œ
           model_output = self.unet(
               sample,
               torch.full(sample.shape[:1], t, dtype=torch.long, device=sample.device),
               global_cond=global_cond
           )
   
           # å™ªå£°è°ƒåº¦å™¨æ­¥è¿›ï¼Œé€šè¿‡è¾“å‡ºçš„å™ªéŸ³ä»¥åŠsampleå¸¦å™ªéŸ³çš„åŠ¨ä½œï¼Œæ¥è¾“å‡ºå»é™¤å™ªéŸ³çš„åŠ¨ä½œsample
           # sampleå»é™¤é¢„æµ‹å‡ºçš„model_outputå™ªå£°ï¼Œå¾—åˆ°æ›´å¥½çš„å¸¦å™ªå£°åŠ¨ä½œ
           # x_{t-1} = 1/âˆšÎ±_t * (x_t - (1-Î±_t)/âˆš(1-á¾±_t) * Îµ_Î¸(x_t, t)) + Ïƒ_t * z
           # sampleå°±æ˜¯è¿™ä¸ªå…¬å¼ä¸­çš„x_{t-1}ï¼ŒÎµ_Î¸æ˜¯model_outputï¼Œx_tæ˜¯ä¸Šä¸€æ­¥çš„sample
           sample = self.noise_scheduler.step(model_output, t, sample).prev_sample
   
       return sample  # [B, horizon, action_dim]
   ```

### å…¶ä½™æ¨¡å—

1. é˜Ÿåˆ—æœºåˆ¶ç»†èŠ‚è¯´æ˜

   ```python
   def populate_queues(queues, batch):
        for key in batch:
            # å¿½ç•¥ä¸åœ¨é˜Ÿåˆ—ä¸­çš„é”®
            if key not in queues:
                continue
            if len(queues[key]) != queues[key].maxlen:
                # åˆå§‹åŒ–ï¼šå¤åˆ¶ç¬¬ä¸€ä¸ªè§‚å¯Ÿå¡«å……é˜Ÿåˆ—
                while len(queues[key]) != queues[key].maxlen:
                    queues[key].append(batch[key])
            else:
                # æ·»åŠ æœ€æ–°è§‚å¯Ÿï¼ˆè‡ªåŠ¨ç§»é™¤æœ€è€çš„ï¼‰
                queues[key].append(batch[key])
        return queues
   ```

   ```
   è§‚å¯Ÿé˜Ÿåˆ— (n_obs_steps=2)ï¼š
   [è§‚å¯Ÿt-1] â†’ [è§‚å¯Ÿt]
                 â†“
   é¢„æµ‹åŠ¨ä½œ (horizon=16)ï¼š
   [åŠ¨ä½œt, åŠ¨ä½œt+1, ..., åŠ¨ä½œt+15]
                 â†“
   æ‰§è¡ŒåŠ¨ä½œ (n_action_steps=8)ï¼š
   [åŠ¨ä½œt, åŠ¨ä½œt+1, ..., åŠ¨ä½œt+7]
   ```

2. FiLMæ¡ä»¶è°ƒåˆ¶æœºåˆ¶

   ```python
   class DiffusionConditionalResidualBlock1d(nn.Module):
      def forward(self, x, cond):
          """
          è¾“å…¥:
            x: [B, in_channels, T] ç‰¹å¾å›¾
            cond: [B, cond_dim] æ¡ä»¶å‘é‡
          è¾“å‡º:
            [B, out_channels, T] æ¡ä»¶è°ƒåˆ¶åçš„ç‰¹å¾
          """
          # å·ç§¯å¤„ç†
          h = self.conv1(x)  # [B, out_channels, T]
   
          # FiLMæ¡ä»¶è°ƒåˆ¶å‚æ•°
          film = self.cond_encoder(cond)  # [B, out_channels] æˆ– [B, out_channels*2]
   
          if self.use_film_scale_modulation:
              # ç¼©æ”¾å’Œåç½®è°ƒåˆ¶
              scale, bias = torch.chunk(film, 2, dim=1)
              h = h * (scale.unsqueeze(-1) + 1) + bias.unsqueeze(-1)
          else:
              # ä»…åç½®è°ƒåˆ¶
              h = h + film.unsqueeze(-1)
   
          # ç¬¬äºŒæ¬¡å·ç§¯
          h = self.conv2(h)
   
          # æ®‹å·®è¿æ¥
          return h + self.residual_conv(x)
   ```

3. UNet

- U-Netæ˜¯æ‰©æ•£æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å­¦ä¹ ä»å™ªå£°è½¨è¿¹ä¸­æ¢å¤åŸå§‹è½¨è¿¹

- é‡‡ç”¨1Då·ç§¯å¤„ç†æ—¶åºåŠ¨ä½œæ•°æ®ï¼Œä¸ä¼ ç»Ÿå›¾åƒå¤„ç†ä¸­çš„2D U-Netç±»ä¼¼

- ä½¿ç”¨åŒè·¯å¾„ç»“æ„ï¼šä¸‹é‡‡æ ·è·¯å¾„å‹ç¼©ç‰¹å¾ï¼Œä¸Šé‡‡æ ·è·¯å¾„è¿˜åŸç»´åº¦

- ä½¿ç”¨è·³è·ƒè¿æ¥ä¿ç•™ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„ç»†èŠ‚ä¿¡æ¯

4. FiLM

- é€šè¿‡FiLMï¼ˆç‰¹å¾çº¿æ€§è°ƒåˆ¶ï¼‰æœºåˆ¶å°†å…¨å±€æ¡ä»¶èå…¥æ¯ä¸ªå·ç§¯å—

  - **FiLM ä¸æ˜¯â€œç›´æ¥æ‹¼æ¥â€ï¼Œè€Œæ˜¯â€œé€šè¿‡æ¡ä»¶æ¥æ§åˆ¶ç½‘ç»œæ¯ä¸€å±‚çš„æ¿€æ´»åˆ†å¸ƒâ€**ï¼Œå®ƒä¸æ˜¯æŠŠæ¡ä»¶ä¿¡æ¯åŠ å…¥åˆ°è¾“å…¥ä¸­ï¼ˆåƒ concat é‚£æ ·ï¼‰ï¼Œè€Œæ˜¯**æ§åˆ¶è¾“å…¥çš„ç‰¹å¾åˆ†å¸ƒ**ã€‚

  ```python
  scale, shift = MLP(condition).chunk(2, dim=-1)
  x = conv(x)
  x = scale.unsqueeze(-1) * x + shift.unsqueeze(-1)
  ```

  - æ­£ç¡®è®¤çŸ¥ï¼šFiLM vs BatchNorm å¯¹æ¯”

    | å±æ€§           | **BatchNorm**                        | **FiLM**                             |
    | -------------- | ------------------------------------ | ------------------------------------ |
    | Î³ / Î² æ¥æº     | âœ… å¯å­¦ä¹ å‚æ•°ï¼Œ**å›ºå®š**ï¼ˆå’Œè¾“å…¥æ— å…³ï¼‰ | âœ… åŠ¨æ€ç”Ÿæˆï¼Œ**æ¥è‡ªæ¡ä»¶è¾“å…¥ï¼ˆcondï¼‰** |
    | æ˜¯å¦éšè¾“å…¥å˜åŒ– | âŒ ä¸å˜åŒ–ï¼ˆÎ³/Î² æ˜¯ learnable çš„å¸¸é‡ï¼‰  | âœ… ä¼šå˜ï¼ˆÎ³/Î² æ˜¯ `MLP(cond)` çš„è¾“å‡ºï¼‰  |
    | æ§åˆ¶ä½œç”¨       | å…¨å±€æ ‡å‡†åŒ– â†’ æ›´å¿«è®­ç»ƒ                | æ¡ä»¶è°ƒæ§ â†’ å¤šä»»åŠ¡ã€å¤šæ¨¡æ€é€‚åº”        |
    | ä½¿ç”¨ä½ç½®       | æ ‡å‡†å±‚åï¼ˆConvã€Linearï¼‰             | æ®‹å·®æ¨¡å—ä¸­æˆ–æ³¨æ„åŠ›ååšè°ƒåˆ¶           |

  ```python
  # BatchNormï¼ˆå›ºå®š Î³/Î²ï¼‰ï¼š
  bn = nn.BatchNorm1d(256)
  x = bn(x)  # Î³ å’Œ Î² æ˜¯æ¨¡å‹å‚æ•°ï¼Œä½†åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šæ˜¯å›ºå®šçš„
  
  # FiLMï¼ˆåŠ¨æ€ Î³/Î²ï¼‰
  film = nn.Linear(cond_dim, 2 * 256)
  scale, shift = film(cond).chunk(2, dim=-1)
  x = scale.unsqueeze(-1) * x + shift.unsqueeze(-1)  # æ¯ä¸ªæ ·æœ¬ä¸åŒ
  ```

5. æ¨ç†æ—¶çš„é‡‡æ ·è¿‡ç¨‹ï¼Œ**è®­ç»ƒå’Œæ¨ç†åœ¨ Diffusion æ¨¡å‹ä¸­æœ¬æ¥å°±æ˜¯ä¸åŒæœºåˆ¶ï¼š**

- æ¨ç†å¿…é¡»ã€Œé€æ­¥åœ°è¿˜åŸå™ªå£° â†’ æ•°æ®ã€

- è®­ç»ƒåªéœ€è¦ã€Œä»ä»»æ„ xtå­¦ä¹  denoise ä¸€æ¬¡ã€å³å¯ï¼Œ**ä¸éœ€è¦å®Œæ•´è·‘ T æ­¥**

- æ¨ç†é˜¶æ®µ = æ¡ä»¶ç”Ÿæˆ + å¤šæ­¥å»å™ªé‡‡æ ·

- å¯¹åº”çš„æ˜¯DDPMä¸­çš„sampleå…¬å¼

- Step 1å‡†å¤‡è¾“å…¥

   1.1 éšæœºåŠ¨ä½œåˆå§‹åŒ–ï¼šä»é«˜æ–¯åˆ†å¸ƒé‡‡æ ·

  ```python
  x_t = torch.randn([B, T, D])  # åˆå§‹çº¯å™ªå£°åŠ¨ä½œ
  ```

  1.2 æå–æ¡ä»¶ä¿¡æ¯ï¼ˆå›¾åƒ + çŠ¶æ€ï¼‰

  ```python
  # å›¾åƒåºåˆ—å¯èƒ½æœ‰å¤šå¸§ï¼Œæ¯å¸§æç‰¹å¾
  image_feats = image_encoder(images)     # [B, feat_dim]
  state_feats = state_encoder(states)     # [B, state_dim]
  
  # æ‹¼æ¥æˆå…¨å±€æ¡ä»¶å‘é‡
  global_cond = torch.cat([image_feats, state_feats], dim=-1)  # [B, G]
  ```

- Step 2ï¼šå¤šæ­¥åå‘å»å™ªé‡‡æ ·ï¼Œä» `t = T` é€æ­¥åˆ° `t = 0`ï¼Œæ¯ä¸€æ­¥éƒ½ç”¨åŒä¸€ä¸ª UNetï¼šè¿™ä¸ª `scheduler.step(...)` å°±æ˜¯å®ç°è®ºæ–‡ä¸­ç»å…¸çš„åå‘å…¬å¼ï¼ˆæ ¹æ® DDPMã€DDIM ç­‰ï¼‰

  ```python
  for t in reversed(range(T)):  # e.g. T = 1000
      t_tensor = torch.full([B], t, dtype=torch.long)
      
      # é¢„æµ‹å½“å‰ x_t çš„å™ªå£°åˆ†é‡
      eps_hat = unet(x_t, t_tensor, global_cond)
      
      # æ‰§è¡Œä¸€è½®åå‘é‡‡æ ·ï¼šx_t â†’ x_{t-1}
      x_t = scheduler.step(eps_hat, t, x_t).prev_sample
  
  ```

- Step 3ï¼šæœ€ç»ˆè¾“å‡ºå¹²å‡€åŠ¨ä½œ

  ```python
  x_0 = x_t  # æœ€åä¸€è½®åçš„ xâ‚€ å°±æ˜¯é¢„æµ‹å‡ºæ¥çš„åŠ¨ä½œåºåˆ—
  ```
